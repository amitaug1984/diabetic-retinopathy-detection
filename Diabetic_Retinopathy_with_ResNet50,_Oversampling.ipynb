{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Diabetic Retinopathy with ResNet50, Oversampling.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "2588aaa7f01f5570afd809a486781cf8592aadfe",
        "id": "AAJVT0m4CyRl",
        "colab_type": "text"
      },
      "source": [
        "# Diabetic Retinopathy \n",
        "\n",
        "Diabetic retinopathy (DR), also known as diabetic eye disease, is a medical condition in which damage occurs to the retina due to diabetes mellitus. It is a leading cause of blindness. Diabetic retinopathy affects up to 80 percent of those who have had diabetes for 20 years or more. Diabetic retinopathy often has no early warning signs. **Retinal (fundus) photography with manual interpretation is a widely accepted screening tool for diabetic retinopathy**, with performance that can exceed that of in-person dilated eye examinations. \n",
        "\n",
        "The below figure shows an example of a healthy patient and a patient with diabetic retinopathy as viewed by fundus photography ([source](https://www.biorxiv.org/content/biorxiv/early/2018/06/19/225508.full.pdf)):\n",
        "\n",
        "![image.png](data:image/jpeg;base64,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)\n",
        "\n",
        "An automated tool for grading severity of diabetic retinopathy would be very useful for accerelating detection and treatment. Recently, there have been a number of attempts to utilize deep learning to diagnose DR and automatically grade diabetic retinopathy. This includes this [competition](https://kaggle.com/c/diabetic-retinopathy-detection) and [work by Google](https://ai.googleblog.com/2016/11/deep-learning-for-detection-of-diabetic.html). Even one deep-learning based system is [FDA approved](https://www.fda.gov/NewsEvents/Newsroom/PressAnnouncements/ucm604357.htm). \n",
        "\n",
        "Clearly, this dataset and deep learning problem is quite well-characterized. \n",
        "\n",
        "# A look at the data:\n",
        "\n",
        "Data description from the competition:\n",
        "\n",
        ">You are provided with a large set of high-resolution retina images taken under a variety of imaging conditions. A left and right field is provided for every subject. >Images are labeled with a subject id as well as either left or right (e.g. 1_left.jpeg is the left eye of patient id 1).\n",
        ">\n",
        ">A clinician has rated the presence of diabetic retinopathy in each image on a scale of 0 to 4, according to the following scale:\n",
        ">\n",
        ">0 - No DR\n",
        ">\n",
        ">1 - Mild\n",
        ">\n",
        ">2 - Moderate\n",
        ">\n",
        ">3 - Severe\n",
        ">\n",
        ">4 - Proliferative DR\n",
        ">\n",
        ">Your task is to create an automated analysis system capable of assigning a score based on this scale.\n",
        "\n",
        "...\n",
        "\n",
        "> Like any real-world data set, you will encounter noise in both the images and labels. Images may contain artifacts, be out of focus, underexposed, or overexposed. A major aim of this competition is to develop robust algorithms that can function in the presence of noise and variation.\n",
        "\n",
        "**A minor problem!**\n",
        "\n",
        "Due to the large image file size, there are **only 1000 files with labels** and one csv file with the labels of all the images in the directory available in the kernel, as demonstrated below. The actual competition had on the order of 35,000 files so this is clearly **a very small subset of the data**. \n",
        "In addition, this is a highly imbalanced dataset. \n",
        "\n",
        "Originally, I had aimed to create a model that would be close to the SOTA, but clearly, with only 1000 images that's not possible.\n",
        "\n",
        "**AIM OF KERNEL:** I will utilize transfer learning, oversampling, and progressive resizing on this small, imbalanced dataset.\n",
        "Given that many real-world datasets are also small and imbalanced, it will be interesting to see how far these techniques will take us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "875c2659dde79c26db51f02c0fe336a89d2aca86",
        "_kg_hide-input": false,
        "_kg_hide-output": false,
        "id": "dcvY36B6CyRp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "9fe97ba1-7e25-435b-dbf0-8a6ed9a11354"
      },
      "source": [
        "from fastai.vision import *\n",
        "import os\n",
        "files = os.listdir('/content/drive/My Drive/input/diabetic-retinopathy-detection')\n",
        "print('trainLabels.csv' in files) #Is the labels csv in the directory?\n",
        "print(len(files)) #There should be 1000 images + 1 csv file = 1001 files"
      ],
      "execution_count": 429,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "251\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "bURe1uigCyR1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 430,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "f3ab6abf99437fba7a29d020c2e9f981da501bf5",
        "_kg_hide-output": true,
        "id": "CnY8ZcAzCyR9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "0715fef3-0c88-45bd-dc0c-3a1ed7d1b688"
      },
      "source": [
        "!pip install fastai==1.0.30"
      ],
      "execution_count": 431,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fastai==1.0.30\n",
            "  Using cached https://files.pythonhosted.org/packages/1c/67/8dd2051bb7dbde20ecca36215bd3e02ed436b976643078e3cdd41bd0256f/fastai-1.0.30-py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.30) (1.18.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.30) (3.2.2)\n",
            "Requirement already satisfied: fastprogress>=0.1.16 in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.30) (0.2.3)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.30) (3.6.6)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torchvision-nightly (from fastai==1.0.30) (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for torchvision-nightly (from fastai==1.0.30)\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "id": "1XYP2CFaCySF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai import *\n",
        "from fastai.vision import *\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 432,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "84a40b83ea5c81e7b9787f90a57dcc5795542485",
        "id": "OuCL0B3iCySM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "1dec5239-7bdd-4076-e777-9455e47f1f47"
      },
      "source": [
        "print('Make sure cuda is installed:', torch.cuda.is_available())\n",
        "print('Make sure cudnn is enabled:', torch.backends.cudnn.enabled)"
      ],
      "execution_count": 433,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Make sure cuda is installed: True\n",
            "Make sure cudnn is enabled: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "9e88f9a81e9613964544b8cbc219dc3c503a3948",
        "id": "e2-O4EuCCySU",
        "colab_type": "text"
      },
      "source": [
        "# Reading data\n",
        "Here I am going to open the dataset with pandas, check distribution of labels, and oversample to reduce imbalance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "9881dcf38a5f61a9c4fc1de79a42b1e956a262c6",
        "id": "egphQghnCySV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "fda188dc-89a6-484f-fe9d-c153cb9dbc41"
      },
      "source": [
        "base_image_dir = os.path.join('/content/drive/My Drive/input/diabetic-retinopathy-detection')\n",
        "df = pd.read_csv(os.path.join(base_image_dir, 'trainLabels.csv'))\n",
        "df['path'] = df['image'].map(lambda x: os.path.join(base_image_dir,'{}.jpeg'.format(x)))\n",
        "df['exists'] = df['path'].map(os.path.exists) #Most of the files do not exist because this is a sample of the original dataset\n",
        "df = df[df['exists']]\n",
        "df = df.drop(columns=['image','exists'])\n",
        "df = df.sample(frac=1).reset_index(drop=True) #shuffle dataframe\n",
        "df.head(10)"
      ],
      "execution_count": 434,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>level</th>\n",
              "      <th>path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>/content/drive/My Drive/input/diabetic-retinop...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>/content/drive/My Drive/input/diabetic-retinop...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>/content/drive/My Drive/input/diabetic-retinop...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>/content/drive/My Drive/input/diabetic-retinop...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>/content/drive/My Drive/input/diabetic-retinop...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>4</td>\n",
              "      <td>/content/drive/My Drive/input/diabetic-retinop...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2</td>\n",
              "      <td>/content/drive/My Drive/input/diabetic-retinop...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>4</td>\n",
              "      <td>/content/drive/My Drive/input/diabetic-retinop...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3</td>\n",
              "      <td>/content/drive/My Drive/input/diabetic-retinop...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2</td>\n",
              "      <td>/content/drive/My Drive/input/diabetic-retinop...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   level                                               path\n",
              "0      3  /content/drive/My Drive/input/diabetic-retinop...\n",
              "1      2  /content/drive/My Drive/input/diabetic-retinop...\n",
              "2      0  /content/drive/My Drive/input/diabetic-retinop...\n",
              "3      3  /content/drive/My Drive/input/diabetic-retinop...\n",
              "4      0  /content/drive/My Drive/input/diabetic-retinop...\n",
              "5      4  /content/drive/My Drive/input/diabetic-retinop...\n",
              "6      2  /content/drive/My Drive/input/diabetic-retinop...\n",
              "7      4  /content/drive/My Drive/input/diabetic-retinop...\n",
              "8      3  /content/drive/My Drive/input/diabetic-retinop...\n",
              "9      2  /content/drive/My Drive/input/diabetic-retinop..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 434
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "cbb0d08b783f23bcd527f4b0891ccdfab9bc84d3",
        "id": "BE68AqlhCySd",
        "colab_type": "text"
      },
      "source": [
        "The dataset is highly imbalanced, with many samples for level 0, and very little for the rest of the levels. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "9ad4b31e5e9ba6a730558bb2234cb633660cb534",
        "id": "JYkTG8VQCySf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "02431c74-17d2-4fc4-c49c-eb902dcbc179"
      },
      "source": [
        "df['level'].hist(figsize = (10, 5))"
      ],
      "execution_count": 435,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f15ab9615f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 435
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "c0326cb22dc2c2c805252e2ed114b39877044c5e",
        "id": "l8F798yuCySn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "779d5bfc-dde0-4225-f5ef-fd66f19454e1"
      },
      "source": [
        "df.pivot_table(index='level', aggfunc=len)"
      ],
      "execution_count": 436,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>level</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       path\n",
              "level      \n",
              "0        50\n",
              "1        50\n",
              "2        50\n",
              "3        50\n",
              "4        50"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 436
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "656266d36031e2615060d6270875e45e7435bc75",
        "id": "2KzF6TwdCySs",
        "colab_type": "text"
      },
      "source": [
        "This is a function to oversample the dataset (so some of the images of levels 1-4 are present multiple times in the dataset):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "edb4fd9ae89665e284058ade1187327d9bf49fc3",
        "id": "Fnd0aH1eCySs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#def balance_data(class_size,df):\n",
        "    #train_df = df.groupby(['level']).apply(lambda x: x.sample(class_size, replace = True)).reset_index(drop = True)\n",
        "    #train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
        "    #print('New Data Size:', train_df.shape[0], 'Old Size:', df.shape[0])\n",
        "    #train_df['level'].hist(figsize = (10, 5))\n",
        "    #return train_df"
      ],
      "execution_count": 437,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "d0e51e417bd35ded42a61c1c760631249dc028d3",
        "id": "hHUtxWmpCySx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(df,test_size=0.2,stratify = df['level']) # Here we will perform an 80%/20% split of the dataset, with stratification to keep similar distribution in validation set"
      ],
      "execution_count": 438,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "68ea6549c1051514d6bb5ba21199bfd4855ddfab",
        "id": "Zvwu5NELCyS1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "8b8643ce-a565-4be5-a168-8f145ce5d3fe"
      },
      "source": [
        "train_df['level'].hist(figsize = (10, 5))\n",
        "len(val_df)"
      ],
      "execution_count": 439,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 439
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "521ee588bf5313ea385999de09703de83f622650",
        "id": "64J9wWc2CyS5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "b971f669-205c-4c65-cb13-f391de8c8b9f"
      },
      "source": [
        "train_df.pivot_table(index='level', aggfunc=len)"
      ],
      "execution_count": 440,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>level</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       path\n",
              "level      \n",
              "0        40\n",
              "1        40\n",
              "2        40\n",
              "3        40\n",
              "4        40"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 440
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "2729b156277ba075c238c4f26dc26ef0f981fa0c",
        "id": "QdquxLnsCyS8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "be2b013e-4a10-4b23-a213-1ea36ed7bdba"
      },
      "source": [
        "#train_df = balance_data(train_df.pivot_table(index='level', aggfunc=len).max().max(),train_df) # I will oversample such that all classes have the same number of images as the maximum\n",
        "train_df['level'].hist(figsize = (10, 5))"
      ],
      "execution_count": 441,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f15ab7bedd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 441
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "b4b24118ac5a0137d0a63c983ff054daa4746536",
        "id": "2k_AbA5gCyS_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "c3a2024d-7f72-4dd0-a060-ff05ba118ce5"
      },
      "source": [
        "df = pd.concat([train_df,val_df]) #beginning of this dataframe is the oversampled training set, end is the validation set\n",
        "len(df)"
      ],
      "execution_count": 442,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "250"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 442
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "c342af8be472dcc5ebfac053572dbe2110eb22c7",
        "id": "7qq9IXOVCyTB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "2b5653c0-adc3-4484-c960-9e7f636362f5"
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "im = Image.open(train_df['path'][1])\n",
        "width, height = im.size\n",
        "print(width,height) "
      ],
      "execution_count": 443,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "512 511\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "b8cdc30e8477775bf8ea5cfe2d54e5a50a9556a6",
        "id": "gtdF6jMbCyTF",
        "colab_type": "text"
      },
      "source": [
        "The images are actually quite big. We will resize to a much smaller size and try to use progressive resizing to our advantage when dealing with such a small dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "cf4e8e212ed4bd9f37c9a7e9bc6e3f02ad3fa777",
        "id": "hG1d9BymCyTF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bs = 16 #smaller batch size is better for training, but may take longer\n",
        "sz=224"
      ],
      "execution_count": 444,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "196aaa8dec9e793975e1cdc9ea560b3779dc7772",
        "id": "5kMfOlgNCyTI",
        "colab_type": "text"
      },
      "source": [
        "Here, I load the dataset into the `ImageItemList` class provided by `fastai`. The fastai library also implements various transforms for data augmentation to improve training. While there are some defaults that I leave intact, I add vertical flipping (`do_flip=True`) and 360 deg. `max_rotate=360` as those have been commonly used for this particular problem.\n",
        "\n",
        "Typically, one would use the `ImageDataBunch` class to load the dataset much easier, but since I needed to do some custom tasks for splitting and oversampling, I have to use this customized creation of the DataBunch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "5ffa729a70f57ffea3542ec4ab80c262bba090de",
        "id": "Z-zlq7JACyTJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "8625fc2d-e6ed-4850-8fc7-0fd55138c823"
      },
      "source": [
        "tfms = get_transforms(do_flip=True,flip_vert=True,max_rotate=360,max_warp=0,max_zoom=1.1,max_lighting=0.1,p_lighting=0.5)\n",
        "src = (ImageItemList.from_df(df=df,path='.',cols='path') #get dataset from dataset\n",
        "        .split_by_idx(range(len(train_df)-1,len(df))) #Splitting the dataset\n",
        "        .label_from_df(cols='level') #obtain labels from the level column\n",
        "      )\n",
        "data= (src.transform(tfms,size=sz) #Data augmentation\n",
        "        .databunch(bs=bs,num_workers=0) #DataBunch\n",
        "        .normalize(imagenet_stats) #Normalize\n",
        "       )"
      ],
      "execution_count": 445,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You can deactivate this warning by passing `no_check=True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/fastai/basic_data.py:226: UserWarning: There seems to be something wrong with your dataset, can't access any element of self.train_ds.\n",
            "Tried: 189,6,116,4,141...\n",
            "  warn(warn_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "fa6ee04a0efbe631dea1d55d370c017b1a2aea7e",
        "id": "NsPCFxLyCyTM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "outputId": "58329667-4874-4276-96d2-e62063bb2caf"
      },
      "source": [
        "data.show_batch(rows=3, figsize=(7,6))"
      ],
      "execution_count": 446,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-446-66824b983385>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/basic_data.py\u001b[0m in \u001b[0;36mshow_batch\u001b[0;34m(self, rows, ds_type, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mDatasetType\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDatasetType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;34m\"Show a batch of data in `ds_type` on a few `rows`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0mn_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrows\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_square_show\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mn_items\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mn_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/basic_data.py\u001b[0m in \u001b[0;36mone_batch\u001b[0;34m(self, ds_type, detach, denorm, cpu)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_detach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mto_detach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/basic_data.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;34m\"Process and returns items from `DataLoader`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/data_block.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idxs)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0midxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtry_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntegral\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m   \u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtfms\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtfmargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/data_block.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idxs)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0midxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtry_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntegral\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxtra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxtra\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/vision/data.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/vision/data.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;34m\"Open image in `fn`, subclass and overwrite for custom behavior.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/vision/image.py\u001b[0m in \u001b[0;36mopen_image\u001b[0;34m(fn, div, convert_mode, cls)\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUserWarning\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# EXIF warning from TiffPlugin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil2tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdiv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2808\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2809\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2810\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '././/content/drive/My Drive/input/diabetic-retinopathy-detection/11889_right.jpeg'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "8800fc0df404e325bb74488d1e90df218666d75b",
        "id": "LgQhV3e1CyTP",
        "colab_type": "text"
      },
      "source": [
        "We can see that because of the center cropping and data augmentation, some of the images are cut off or have weird artifacts. This is probably because the fundus image was not 100% centered in the original data. For now, I will ignore these problems."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "9675456a91e8bef188ba3329864057c8e04ed935",
        "id": "oqohxQp7CyTQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(data.classes)\n",
        "len(data.classes),data.c"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "1dbea921d4ad7da5e234f55da26b052a68f2b689",
        "id": "-gzB0HGbCyTT",
        "colab_type": "text"
      },
      "source": [
        "# Training (Transfer learning) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "918e83963def0ac49984094ea485fe24f247af5f",
        "id": "CHJAD6S4CyTU",
        "colab_type": "text"
      },
      "source": [
        "The Kaggle competition used the Cohen's quadratically weighted kappa so I have that here to compare. This is a better metric when dealing with imbalanced datasets like this one, and for measuring inter-rater agreement for categorical classification (the raters being the human-labeled dataset and the neural network predictions). Here is an implementation based on the scikit-learn's implementation, but converted to a pytorch tensor, as that is what fastai uses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "29ce1b9493c55e77b72bdc32f9fbf0c20926ba64",
        "id": "R3Jsqq3zCyTU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "def quadratic_kappa(y_hat, y):\n",
        "    return torch.tensor(cohen_kappa_score(torch.argmax(y_hat,1), y, weights='quadratic'),device='cuda:0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "c251dde59220e5e6c4b6713bbacc9b57fd8d6348",
        "id": "aFvwGrDkCyTX",
        "colab_type": "text"
      },
      "source": [
        "**Training:**\n",
        "\n",
        "We use transfer learning, where we retrain the last layers of a pretrained neural network. I use the ResNet50 architecture trained on the ImageNet dataset, which has been commonly used for pre-training applications in computer vision. Fastai makes it quite simple to create a model and train:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "3d0f3de01697fa7cce617ce6c8a272dd15182e6b",
        "id": "TjI6daILCyTY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision\n",
        "learn = create_cnn(data, models.resnet50, metrics = [accuracy,quadratic_kappa])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "eb9c7c8041f97055da560dca2c62f73a90aebe83",
        "id": "IqAJ_cbMCyTb",
        "colab_type": "text"
      },
      "source": [
        "We use the learning-rate finder developed by Dr. Leslie Smith and implemented by the fastai team in their library:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "7d23a85a03ba19df350f83846fd9da347770b8d6",
        "id": "ts1h-j8ECyTb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.lr_find()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "30547392f58587773e7b3257c41574042e9b6fa1",
        "id": "_aChjGQ7CyTf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.recorder.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "fce816821161d22f40e284bb8204f88c90a3e09f",
        "id": "WciMX3zoCyTj",
        "colab_type": "text"
      },
      "source": [
        "Here we can see that the loss decreases fastest around `lr=2e-3` so that is what we will use to train: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "24f2e105338b12c64794f60282245caaccd0b5e7",
        "id": "xCPZtDsNCyTk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.fit_one_cycle(4,max_lr = 2e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "7de59881525839b54c5be644c745ff66ccd13e89",
        "id": "EM6XbABnCyTo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.recorder.plot_losses()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "c178f23ff022d296fb2c23acdca88da8e0730fac",
        "id": "RNRhsjolCyTq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.save('stage-1-224')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "6082049e7806d256e5fba28a81c662f5798d24c1",
        "id": "omMIDq7CCyTt",
        "colab_type": "text"
      },
      "source": [
        "The previous model only trained the last model. We can unfreeze the rest of the model, and train the rest of the model using discriminative learning rates. The first layers aren't changed as much, with lower learning rates, while the last layers are changed more, with higher learning rates. We use the learning rate finder again, and use a range of learning rates for different layers in the neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "80c41c7e446ac2bb0863e16ef224f807a24014b7",
        "id": "9FqH-dh3CyTu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.unfreeze()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "3afbafcd3acb792af00422be12dac6b147691ca9",
        "id": "Nz6QtnBJCyTx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.lr_find()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "e3a79af7f052b99c22c7c3ed5847a66b56b83918",
        "id": "w4YwZ6PTCyT1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.recorder.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "3d2f180fc85239106097d06bc909db5f8e8ebfbf",
        "id": "7CMGm53FCyT3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.fit_one_cycle(4, max_lr=slice(1e-6,1e-4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "efbf51af0dda825cf335541b0d26a150a24902df",
        "id": "Th-im0KICyT6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.recorder.plot_losses()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "49b0c6d09518e687547346a8c8248c29bd8be866",
        "id": "GVy1BGA0CyT8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.save('stage-2-224')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "a6382e068d7a8107490ac81924515eb6db98bc26",
        "id": "fIZgp97nCyT_",
        "colab_type": "text"
      },
      "source": [
        "# Progressive resizing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "fc6a594bb38fb327c12919aa99620c3426b663f9",
        "id": "ptqHxLPKCyUA",
        "colab_type": "text"
      },
      "source": [
        "Progressive resizing is a technique developed by Jeremy Howard as part of the fast.ai class and for the [DAWNBench challenge](https://www.fast.ai/2018/08/10/fastai-diu-imagenet/). The idea is that we train with smaller images at the beginning, and retrain with larger images, which will have more information to learn from. This could be very helpful for dealing with small datasets, and I decided to give this a try. I only resize once due to kernel time limitations, but theoretically, we could continue to resize and see if that improves accuracy, especially since the retinal images are so big in size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "dbf0134e5d48674b1832d8f58884c6202cbb0a93",
        "id": "dYAP0JlRCyUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = (src.transform(tfms,size=sz*2) #Data augmentation\n",
        "        .databunch(bs=bs,num_workers=0) #DataBunch\n",
        "        .normalize(imagenet_stats) #Normalize\n",
        "       )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "4e57e0973a8d2a74151c832f258974ff543cbd8c",
        "id": "7dix3TXzCyUD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.data = data\n",
        "data.train_ds[0][0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "254c97b12e684d2aef9d32b3aea6b503b57bebda",
        "id": "u0w8Co-aCyUG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.freeze()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "21eea8553d0154819d6e463368e5fbfba73d1f82",
        "id": "Lcn5gpi_CyUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.lr_find()\n",
        "learn.recorder.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "ef2721e6c1ca81aac47a6372509367a32ec3a09d",
        "id": "_f5Ef6L0CyUL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.fit_one_cycle(4,max_lr=1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "eb6dd8b185204fdd271b3604f110aba6e44c305f",
        "id": "YWeeSqvSCyUO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.recorder.plot_losses()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "f6b68eded13776b3e92f3d1796676d93576aac8f",
        "id": "FEXAUwymCyUS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.save('stage-1-448')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "a7586f03ae3a6f9c8fec19242be469f2d1ba4b07",
        "id": "6akER_pLCyUV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.unfreeze()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "cc37b8df83870d41d4cbfb4b49d53284d6e418cd",
        "id": "dBEVnar-CyUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.lr_find()\n",
        "learn.recorder.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "502f64326608cf5c20d5c6ef1cd399232bad5023",
        "id": "jXWg1kZOCyUf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.fit_one_cycle(4,max_lr=slice(1e-6,1e-4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "771ae7cc7d6272689b155119de4af5a07aa601f0",
        "id": "W7YNB0c4CyUi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.recorder.plot_losses()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "0e4b1b24ab8783d6d254f9d08cc9c184b4826a90",
        "id": "NLJUyDz5CyUl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.save('stage-2-448')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "444040e745f7685c3dd6d3a4121984acef62c75d",
        "id": "iAOcPsH4CyUo",
        "colab_type": "text"
      },
      "source": [
        "# Checking results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "cf99796e2197d141a0a7d640552b7e2cdcdc5261",
        "id": "8O90jm4KCyUp",
        "colab_type": "text"
      },
      "source": [
        "We look at our predictions and make a confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "cb2ead6c938d4c4f197574296fc86f808622a4a0",
        "id": "xt7JMXXACyUp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "interp = ClassificationInterpretation.from_learner(learn)\n",
        "\n",
        "losses,idxs = interp.top_losses()\n",
        "\n",
        "len(data.valid_ds)==len(losses)==len(idxs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "0f3c64f4fa079487a1f15d1ba7b470c75081da60",
        "id": "BC2d9kvVCyUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "interp.plot_top_losses(9, figsize=(15,11))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "dc99e0f8ba533c0db635ee7d1443e6082a32963f",
        "id": "kpIcJQMeCyUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "interp.plot_confusion_matrix(figsize=(12,12), dpi=60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "2719c30c3e10e97913d40873036d4e4d9b3a2985",
        "id": "QKV87yIlCyUw",
        "colab_type": "text"
      },
      "source": [
        "Quite impressively, with only 1000 images, oversampling, and transfer learning, we acheive decent agreement with the correct labels! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "c3ef1801e985e35e09d1771abd0cd06e44918ebd",
        "id": "9JTkCTY8CyUx",
        "colab_type": "text"
      },
      "source": [
        "# Future work:\n",
        "There are other methods for dealing with imbalanced datasets. The most common alternative is to use a **weighted loss function**, with the class weights dependent on the distribution of the dataset.\n",
        "\n",
        "Some possible future experiments include:\n",
        "1. Training the dataset without the healthy (0) class, then retrain with the healthy class.\n",
        "2. Possibly using [mixup](https://forums.fast.ai/t/mixup-data-augmentation/22764) as a form of data augmentation. It would be interesting to see how well it would perform with medical data.\n",
        "3. Utilize [rectangular images](https://www.fast.ai/2018/08/10/fastai-diu-imagenet/) instead of cropping. Most of the data has an aspect ratio of ~1.5, and by default, the `fastai` library center-crops squares to pass into the network. Note for this dataset, it may not provide significant advantage as the circular retinal images are already padded, and cropping them removes much of the padding. \n",
        "\n",
        "# Acknowledgements\n",
        "Thanks to the `fastai` library and [fast.ai course](https://course.fast.ai/). The techniques used here were based mainly on lesson 1 and lesson 3 of the course."
      ]
    }
  ]
}